# Part 1: 이론 및 환경 설정 (40분)

---

## Slide 1: 타이틀 슬라이드
```
╔══════════════════════════════════════════════════════════╗
║                                                          ║
║     YOLO ROI 실시간 사람 감지 시스템 구축 완전 가이드    ║
║                                                          ║
║            Building Real-Time Person Detection          ║
║               with YOLO and ROI System                   ║
║                                                          ║
║                      강사: [이름]                        ║
║                   소요 시간: 3시간                       ║
║                                                          ║
╚══════════════════════════════════════════════════════════╝
```

**강의 스크립트**:
> "안녕하세요! 오늘 우리는 3시간 동안 YOLO를 활용한 실시간 사람 감지 시스템을 처음부터 끝까지 만들어볼 것입니다. 이 강의는 단순한 이론 강의가 아니라, 실제로 작동하는 완전한 시스템을 여러분의 손으로 직접 구축하는 실습 중심 강의입니다."

---

## Slide 2: 강의 목표
```
🎯 이 강의를 통해 여러분은...

✅ YOLO 객체 탐지 알고리즘의 원리와 활용법을 이해합니다

✅ ROI(관심 영역) 기반 실시간 감지 시스템을 구축합니다

✅ Streamlit을 활용한 웹 UI 대시보드를 개발합니다

✅ RK3588, Jetson Orin 등 임베디드 플랫폼에 배포합니다

✅ 성능 최적화로 40-60% 리소스를 절감합니다
```

**강의 스크립트**:
> "오늘 강의의 목표는 다섯 가지입니다. 첫째, YOLO 알고리즘을 이해하고 실전에 활용하는 방법을 배웁니다. 둘째, 특정 영역만 감시하는 ROI 시스템을 만듭니다. 셋째, 사용하기 쉬운 웹 UI를 개발합니다. 넷째, 임베디드 보드에 실제로 배포해봅니다. 마지막으로, 성능을 최적화해서 리소스 사용을 절반 가까이 줄이는 방법을 배웁니다."

---

## Slide 3: 완성된 시스템 데모
```
🎬 완성된 시스템 미리보기

┌─────────────────────────────────────────────┐
│  📹 실시간 카메라 화면                       │
│                                             │
│  [ROI1-녹색]        [ROI2-빨간색]           │
│   ┌────┐                                    │
│   │👤  │  Person 0.95                       │
│   └────┘                                    │
│                                             │
│  FPS: 30.0 | Detection: 1.0 FPS             │
└─────────────────────────────────────────────┘

📊 ROI별 상태 모니터링
  ROI1: ✅ 검출 (5초) → API 전송 완료
  ROI2: ❌ 미검출 (1초)
  ROI3: ✅ 검출 (3초)
```

**강의 스크립트**:
> "먼저 완성된 시스템이 어떻게 작동하는지 보여드리겠습니다. [실제 데모 시연] 보시는 것처럼, 실시간으로 카메라 화면에서 사람을 감지하고, 특정 영역에 사람이 들어오면 녹색으로 표시됩니다. 오른쪽 ROI에는 사람이 없으니 빨간색이죠. 파란색 박스가 실제 사람이 검출된 위치이고, 신뢰도가 0.95로 표시됩니다. 그리고 5초 이상 지속되면 자동으로 API로 이벤트를 전송합니다."

---

## Slide 4: 실제 활용 사례
```
🏢 실전 활용 사례

1️⃣ 스마트 빌딩 보안
   - 특정 구역 무단 침입 감지
   - 출입 통제 구역 모니터링

2️⃣ 소매점 고객 분석
   - 특정 진열대 고객 체류 시간
   - 동선 분석 및 관심 영역 파악

3️⃣ 공장 안전 관리
   - 위험 구역 작업자 감지
   - 안전모/보호복 착용 확인

4️⃣ 주차장 관리
   - 불법 주차 감지
   - 주차 공간 점유 현황

5️⃣ 응급 상황 감지
   - 낙상 사고 자동 감지
   - 장시간 움직임 없음 알림
```

**강의 스크립트**:
> "이 시스템은 실제로 다양한 분야에서 활용될 수 있습니다. 빌딩 보안에서는 특정 구역에 무단 침입을 감지하고, 소매점에서는 고객이 어느 진열대에 얼마나 머무는지 분석합니다. 공장에서는 위험 구역에 작업자가 들어가면 즉시 경보를 울리고, 주차장에서는 불법 주차를 자동으로 감지할 수 있습니다. 또한 독거노인 가정에서 낙상 사고나 장시간 움직임이 없을 때 자동으로 알림을 보낼 수도 있습니다."

---

## Slide 5: 강의 일정 (3시간)
```
⏰ 강의 일정

Part 1 (40분) 📚 이론 및 환경 설정
  ├─ YOLO 객체 탐지 기초
  ├─ 시스템 아키텍처 설계
  └─ 개발 환경 구축

Part 2 (80분) 💻 핵심 기능 구현
  ├─ 카메라 입력 & YOLO 통합
  ├─ ROI 시스템 구현
  ├─ 상태 관리 & API 연동
  └─ 백그라운드 스레드 최적화

Part 3 (30분) 🌐 Streamlit 웹 UI 개발
  ├─ UI 레이아웃 구성
  ├─ ROI 편집 인터페이스
  └─ 실시간 대시보드

Part 4 (20분) 🚀 플랫폼별 배포
  ├─ x86_64, RK3588, Jetson Orin
  └─ TensorRT 최적화

Part 5 (20분) 🔧 고급 기능 & 문제 해결

Part 6 (10분) 💬 마무리 & Q&A
```

**강의 스크립트**:
> "오늘 강의는 총 6개 파트로 구성되어 있습니다. 먼저 40분 동안 이론을 배우고 환경을 설정합니다. 그 다음 80분, 가장 긴 시간을 핵심 기능 구현에 할애합니다. 여기서 실제로 코드를 작성하고 테스트합니다. 그리고 30분 동안 사용자 친화적인 웹 UI를 만듭니다. 이어서 20분간 실제 하드웨어에 배포하는 방법을 배우고, 20분 동안 성능 최적화와 문제 해결을 다룹니다. 마지막 10분은 여러분의 질문에 답변하는 시간입니다."

---

## Slide 6: YOLO란 무엇인가?
```
🤖 YOLO (You Only Look Once)

"한 번만 봐도 알 수 있다"

전통적 방법                    YOLO
──────────────────────────────────────
1. 영역 제안 (Region Proposal)  ❌ 불필요
2. 각 영역 분류                 ❌ 불필요
3. 바운딩 박스 조정              ✅ 한 번에!

💡 핵심 아이디어:
   이미지 전체를 한 번의 신경망 통과로 처리
   → 빠르고 정확한 실시간 객체 탐지

📈 발전 과정:
   YOLOv1 (2015) → ... → YOLOv8 (2023) → YOLOv11 (2024)
   
⚡ YOLOv8 특징:
   • 정확도 향상 (mAP ↑)
   • 속도 최적화 (FPS ↑)
   • 사용 편의성 (Ultralytics 패키지)
```

**강의 스크립트**:
> "YOLO는 'You Only Look Once', 즉 '한 번만 봐도 알 수 있다'는 뜻입니다. 전통적인 객체 탐지는 이미지를 여러 번 분석해야 했습니다. 먼저 물체가 있을 만한 영역을 제안하고, 각 영역을 분류하고, 박스를 조정하는 3단계 과정이었죠. 하지만 YOLO는 이 모든 과정을 단 한 번의 신경망 통과로 끝냅니다. 그래서 실시간 처리가 가능한 겁니다. 우리가 오늘 사용할 YOLOv8은 2023년에 나온 최신 버전으로, 정확도와 속도가 모두 뛰어나고, Ultralytics 패키지 덕분에 사용하기도 매우 쉽습니다."

---

## Slide 7: 객체 탐지 핵심 개념
```
🎯 객체 탐지의 핵심 개념

1️⃣ Bounding Box (BBox)
   ┌──────────────┐
   │   👤         │  ← 객체를 감싸는 직사각형
   │              │     (x1, y1, x2, y2)
   └──────────────┘

2️⃣ Confidence Score
   Person: 0.95  ← 95% 확신
   Person: 0.42  ← 42% 확신 (낮음)
   
   Threshold = 0.5 → 0.5 이상만 표시

3️⃣ Class Label
   COCO Dataset: 80개 클래스
   - Person (우리가 사용)
   - Car, Dog, Cat, ...

4️⃣ NMS (Non-Maximum Suppression)
   같은 객체에 여러 박스 → 가장 좋은 것만 선택
   
   Before NMS:      After NMS:
   ┌──┬──┐          ┌──────┐
   │  │  │    →     │      │
   └──┴──┘          └──────┘
```

**강의 스크립트**:
> "객체 탐지를 이해하려면 네 가지 핵심 개념을 알아야 합니다. 첫째, 바운딩 박스는 객체를 감싸는 직사각형입니다. 좌상단과 우하단 좌표로 표현됩니다. 둘째, 신뢰도 점수는 AI가 얼마나 확신하는지를 나타냅니다. 0.95면 95% 확신, 0.42면 42%만 확신하는 거죠. 우리는 보통 0.5 이상만 표시합니다. 셋째, 클래스 라벨은 객체의 종류입니다. YOLO는 80가지를 인식하는데, 우리는 'Person' 클래스만 사용합니다. 마지막으로 NMS는 같은 객체에 여러 박스가 겹칠 때, 가장 좋은 박스 하나만 선택하는 기법입니다."

---

## Slide 8: 우리 시스템의 아키텍처
```
🏗️ 시스템 아키텍처

┌─────────────────────────────────────────────────┐
│                Streamlit Web UI                 │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
│  │ROI 편집  │  │실시간검출│  │통계/로그│     │
│  └──────────┘  └──────────┘  └──────────┘     │
└─────────────────────────────────────────────────┘
          ↕ (Queue 통신)
┌─────────────────────────────────────────────────┐
│         RealtimeDetector (백그라운드 스레드)     │
│                                                 │
│  Camera → YOLO → ROI Check → State → API       │
│    ↓       ↓        ↓           ↓       ↓      │
│  OpenCV  YOLOv8  Polygon   Dwell Time  POST    │
└─────────────────────────────────────────────────┘
          ↕
┌─────────────────────────────────────────────────┐
│              Hardware Layer                     │
│  • USB Camera                                   │
│  • x86_64 / RK3588 / Jetson Orin               │
│  • CPU / GPU / TensorRT                        │
└─────────────────────────────────────────────────┘
```

**강의 스크립트**:
> "우리 시스템은 3계층 아키텍처로 구성됩니다. 맨 위는 사용자가 보는 Streamlit 웹 UI입니다. ROI를 편집하고, 실시간 검출을 보고, 통계를 확인할 수 있죠. 중간은 백그라운드 스레드에서 돌아가는 실제 탐지 엔진입니다. 카메라에서 프레임을 받아서, YOLO로 사람을 찾고, ROI 안에 있는지 확인하고, 상태를 업데이트하고, 필요하면 API로 전송합니다. 맨 아래는 하드웨어 계층으로, 카메라와 컴퓨팅 플랫폼입니다. 중요한 점은 UI와 탐지 엔진이 Queue로 통신한다는 것입니다. 그래서 UI가 느려도 탐지는 계속 빠르게 돌아갑니다."

---

## Slide 9: 개발 환경 준비물
```
💻 필수 준비물

🖥️ 하드웨어:
  ✅ 개발용 PC (권장: i5 이상, 8GB RAM+)
  ✅ 웹캠 또는 USB 카메라
  ✅ (선택) Jetson Orin / RK3588 보드

🐍 소프트웨어:
  ✅ Python 3.8 - 3.11
  ✅ pip (패키지 관리자)
  ✅ Git
  ✅ VS Code 또는 텍스트 에디터

📦 주요 패키지:
  • ultralytics>=8.0.0    (YOLO)
  • opencv-python>=4.8.0  (카메라)
  • streamlit>=1.28.0     (UI)
  • numpy>=1.24.0         (배열)
  • requests>=2.31.0      (API)

🧠 사전 지식:
  ✅ Python 기초 문법
  ✅ 터미널/명령 프롬프트 사용
  ❓ OpenCV (배우면서 진행)
  ❓ 머신러닝 (선택사항)
```

**강의 스크립트**:
> "이제 실습을 시작하기 전에 필요한 것들을 확인해봅시다. 하드웨어로는 일반 PC면 충분합니다. i5 프로세서에 8GB RAM 정도면 됩니다. 웹캠이나 USB 카메라는 필수입니다. Jetson이나 RK3588 보드는 선택사항입니다. 소프트웨어는 Python 3.8부터 3.11 버전이 필요합니다. 3.12는 아직 일부 패키지가 호환되지 않아요. pip와 Git은 필수이고, VS Code 같은 에디터가 있으면 편합니다. 주요 패키지는 ultralytics, opencv, streamlit 등인데, 곧 한꺼번에 설치할 겁니다. 사전 지식으로는 Python 기초만 알면 됩니다. OpenCV나 머신러닝은 몰라도 괜찮습니다. 진행하면서 필요한 부분만 배울 겁니다."

---

## Slide 10: 환경 설정 - GitHub 클론
```
🔧 Step 1: GitHub 리포지토리 클론

터미널/명령 프롬프트를 열고:

# 1. 프로젝트 다운로드
git clone https://github.com/futurianh1k/roidetyolo.git

# 2. 디렉토리 이동
cd roidetyolo

# 3. 파일 확인
ls -l
(Windows: dir)

📁 주요 파일 구조:
roidetyolo/
├── streamlit_app.py          ← Streamlit 메인 앱
├── realtime_detector.py      ← 실시간 탐지 엔진
├── camera_utils.py           ← 카메라 유틸리티
├── roi_utils.py              ← ROI 관리
├── requirements.txt          ← 패키지 목록
├── config.json               ← 설정 파일
└── README.md                 ← 프로젝트 설명
```

**강의 스크립트**:
> "자, 이제 실습을 시작합니다. 먼저 터미널을 여세요. Windows는 명령 프롬프트나 PowerShell, Mac은 터미널입니다. [실제 화면 시연] 이 명령어로 GitHub에서 프로젝트를 다운로드합니다. 그리고 cd 명령어로 폴더에 들어갑니다. ls나 dir로 파일을 확인해보면, streamlit_app.py가 메인 파일이고, realtime_detector.py가 탐지 엔진입니다. camera_utils는 카메라 관련 기능, roi_utils는 ROI 관리 기능이 들어있습니다. requirements.txt는 필요한 패키지 목록이고, config.json은 설정 파일입니다."

---

## Slide 11: 환경 설정 - 패키지 설치
```
🔧 Step 2: Python 패키지 설치

# Python 버전 확인
python --version
또는
python3 --version

# 패키지 설치 (일반 환경)
pip install -r requirements.txt

# 또는 개별 설치
pip install ultralytics opencv-python streamlit numpy requests

⏱️ 설치 시간: 약 2-3분

⚠️ 주의사항:
• NumPy 2.0 이상은 호환 문제 → numpy<2.0.0
• Python 3.12는 일부 패키지 미지원
• GPU 사용 시 PyTorch CUDA 버전 필요

✅ 설치 확인:
python -c "from ultralytics import YOLO; print('✅ YOLO 설치 완료')"
```

**강의 스크립트**:
> "패키지 설치는 아주 간단합니다. 먼저 Python 버전을 확인하세요. python --version 또는 python3 --version을 입력합니다. 3.8부터 3.11 사이여야 합니다. 그 다음 pip install -r requirements.txt를 실행하면 필요한 모든 패키지가 자동으로 설치됩니다. 2-3분 정도 걸립니다. 주의할 점은 NumPy 2.0 이상은 호환 문제가 있으니 2.0 미만으로 설치되어야 하고, Python 3.12는 아직 일부 패키지가 지원되지 않습니다. GPU를 사용하려면 PyTorch CUDA 버전이 필요한데, 이건 나중에 Jetson 파트에서 다룹니다. 설치가 끝나면 이 명령어로 확인할 수 있습니다."

---

## Slide 12: 환경 설정 - 카메라 테스트
```
🔧 Step 3: 카메라 연결 테스트

# 카메라 감지 테스트
python test_camera_detection.py

예상 출력:
╔══════════════════════════════════════════════╗
║        사용 가능한 카메라 목록                ║
╠══════════════════════════════════════════════╣
║ [0] USB Camera                               ║
║     해상도: 1280x720                         ║
║     FPS: 30                                  ║
║     백엔드: V4L2 (Linux) / DirectShow (Win)  ║
╚══════════════════════════════════════════════╝

🔍 문제 해결:
❌ "카메라를 찾을 수 없습니다"
   → USB 케이블 확인
   → 다른 앱에서 카메라 사용 중인지 확인
   → Linux: 권한 문제 (./check_camera_permissions.sh)

❌ "Permission denied"
   → sudo usermod -aG video $USER
   → 로그아웃 후 재로그인
```

**강의 스크립트**:
> "패키지 설치가 끝났으면 카메라를 테스트합니다. test_camera_detection.py를 실행하면 연결된 카메라를 자동으로 찾아줍니다. [실제 실행 화면] 보시는 것처럼 카메라 번호, 해상도, FPS, 백엔드 정보가 표시됩니다. 만약 '카메라를 찾을 수 없습니다'라고 나오면, USB 케이블을 확인하고, 다른 프로그램에서 카메라를 사용하고 있지는 않은지 확인하세요. Linux에서는 권한 문제가 있을 수 있습니다. 이 경우 check_camera_permissions.sh 스크립트를 실행하거나, usermod 명령어로 video 그룹에 사용자를 추가해야 합니다."

---

## Slide 13: Part 1 요약
```
✅ Part 1 완료! 지금까지 배운 것:

📚 이론:
  ✓ YOLO 객체 탐지 알고리즘 원리
  ✓ Confidence, BBox, NMS 개념
  ✓ 시스템 아키텍처 설계

🔧 실습:
  ✓ GitHub 프로젝트 클론
  ✓ Python 패키지 설치
  ✓ 카메라 연결 및 테스트

🎯 다음 Part 2에서는:
  → 카메라 프레임 읽기
  → YOLO로 사람 탐지
  → ROI 시스템 구현
  → API 연동

⏸️ 5분 휴식 후 Part 2로 이어집니다!
```

**강의 스크립트**:
> "Part 1이 끝났습니다! 지금까지 우리는 YOLO가 무엇인지, 어떻게 작동하는지 배웠고, 시스템 전체 구조를 이해했습니다. 그리고 실제로 개발 환경을 구축했습니다. GitHub에서 프로젝트를 다운로드하고, 필요한 패키지를 설치하고, 카메라가 제대로 작동하는지 확인했습니다. 다음 Part 2는 오늘 강의의 핵심입니다. 80분 동안 실제로 코드를 작성하고 테스트할 겁니다. 카메라에서 프레임을 읽고, YOLO로 사람을 찾고, ROI를 만들고, API로 데이터를 보내는 모든 과정을 직접 해볼 겁니다. 자, 5분 휴식 후에 Part 2로 넘어가겠습니다!"

---

## 추가 슬라이드 참고자료

### Slide 14: 참고 링크
```
🔗 유용한 참고 자료

📖 공식 문서:
  • YOLO: https://docs.ultralytics.com
  • Streamlit: https://docs.streamlit.io
  • OpenCV: https://docs.opencv.org

💻 프로젝트:
  • GitHub: https://github.com/futurianh1k/roidetyolo
  • Issues: 문제가 있으면 여기에 올리세요!

📚 추가 학습:
  • YOLOv8 Tutorial
  • Computer Vision Basics
  • Python Threading
```

---

**Part 1 슬라이드 종료 - 총 14장**
