# ğŸ”¬ ì–¼êµ´ ë¶„ì„ ì‹œìŠ¤í…œ í†µí•© ê°€ì´ë“œ (Jetson Orin ìµœì í™”)

---

## ğŸ“‹ ê°œìš”

**ëª©í‘œ**: ê¸°ì¡´ YOLO ROI ì‚¬ëŒ ê°ì§€ ì‹œìŠ¤í…œì— ì–¼êµ´ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€

**ë¶„ì„ í•­ëª©**:
1. ì–¼êµ´ í‘œì • (Facial Expression) - ê°ì •, ê³ í†µ, ì°¡ê·¸ë¦¼
2. ëˆˆ ìƒíƒœ (Eye State) - ëœ¨ê¸°/ê°ê¸°
3. ì… ìƒíƒœ (Mouth State) - ì—´ê¸°/ë‹«ê¸°/ë§í•˜ê¸°
4. ì¸ê³µí˜¸í¡ê¸° ê²€ì¶œ (Ventilator Detection)

**í”Œë«í¼**: Jetson Orin (GPU ê°€ì†)

---

## ğŸ¯ ì¶”ì²œ ì•„í‚¤í…ì²˜ (Jetson Orin ìµœì í™”)

### **2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: ì‚¬ëŒ ê²€ì¶œ (YOLO)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ì…ë ¥: ì¹´ë©”ë¼ í”„ë ˆì„ (1280x720)                          â”‚
â”‚  ì¶œë ¥: ì‚¬ëŒ BBox + ROI í•„í„°ë§                            â”‚
â”‚  ì„±ëŠ¥: 30-60 FPS (í˜„ì¬ ì‹œìŠ¤í…œ)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (ì‚¬ëŒ BBox ì „ë‹¬)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: ì–¼êµ´ ë¶„ì„ (MediaPipe Face Mesh)               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ì…ë ¥: ì‚¬ëŒ BBox í¬ë¡­ ì´ë¯¸ì§€                             â”‚
â”‚  ì²˜ë¦¬:                                                   â”‚
â”‚    1. Face Detection (ì–¼êµ´ ê²€ì¶œ)                        â”‚
â”‚    2. Face Landmarks (468ê°œ íŠ¹ì§•ì )                     â”‚
â”‚    3. Feature Analysis:                                 â”‚
â”‚       - EAR (Eye Aspect Ratio) â†’ ëˆˆ ê°œí              â”‚
â”‚       - MAR (Mouth Aspect Ratio) â†’ ì… ìƒíƒœ            â”‚
â”‚       - Facial Expression â†’ í‘œì • ë¶„ë¥˜                  â”‚
â”‚       - Mask/Ventilator Detection â†’ ë§ˆìŠ¤í¬/í˜¸í¡ê¸°     â”‚
â”‚  ì„±ëŠ¥: 15-30 FPS (ì–¼êµ´ë‹¹)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ ì™œ MediaPipe Face Meshì¸ê°€?

### âœ… ì¥ì 
1. **ê²½ëŸ‰**: CPUì—ì„œë„ 30+ FPS
2. **ì •í™•**: 468ê°œ 3D ëœë“œë§ˆí¬
3. **ë¬´ë£Œ**: ì˜¤í”ˆì†ŒìŠ¤ (Apache 2.0)
4. **GPU ê°€ì†**: TensorFlow Lite GPU ì§€ì›
5. **ê²€ì¦ë¨**: Google í”„ë¡œë•ì…˜ ì‚¬ìš©

### âŒ ëŒ€ì•ˆë“¤ì˜ ë‹¨ì 
- **dlib**: CPUë§Œ ì§€ì›, ëŠë¦¼ (5-10 FPS)
- **OpenCV DNN**: ì œí•œì  ëœë“œë§ˆí¬ (68ê°œ)
- **ì»¤ìŠ¤í…€ CNN**: í•™ìŠµ ë°ì´í„° ë¶€ì¡±, ê°œë°œ ì‹œê°„ â†‘

---

## ğŸ”§ êµ¬í˜„ ë°©ë²•

### 1ï¸âƒ£ **íŒ¨í‚¤ì§€ ì„¤ì¹˜**

```bash
# MediaPipe ì„¤ì¹˜
pip install mediapipe-gpu opencv-python numpy

# ì–¼êµ´ í‘œì • ë¶„ì„ìš© (ì„ íƒ)
pip install fer  # Facial Expression Recognition
```

### 2ï¸âƒ£ **FaceAnalyzer í´ë˜ìŠ¤ ì„¤ê³„**

```python
import mediapipe as mp
import cv2
import numpy as np
from collections import deque

class FaceAnalyzer:
    """
    MediaPipe ê¸°ë°˜ ì‹¤ì‹œê°„ ì–¼êµ´ ë¶„ì„ê¸°
    - ëˆˆ ê°œí (EAR)
    - ì… ìƒíƒœ (MAR)
    - í‘œì • ë¶„ì„
    - ë§ˆìŠ¤í¬/í˜¸í¡ê¸° ê²€ì¶œ
    """
    
    def __init__(self):
        # MediaPipe Face Mesh ì´ˆê¸°í™”
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=5,           # ìµœëŒ€ 5ëª…
            refine_landmarks=True,      # ëˆˆ/ì…ìˆ  ì •ì œ
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (468ê°œ ì¤‘ ì£¼ìš” ì )
        self.LEFT_EYE = [362, 385, 387, 263, 373, 380]
        self.RIGHT_EYE = [33, 160, 158, 133, 153, 144]
        self.MOUTH_OUTER = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308]
        self.MOUTH_INNER = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415]
        
        # ì„ê³„ê°’
        self.EAR_THRESHOLD = 0.21      # ëˆˆ ê°ìŒ ê¸°ì¤€
        self.MAR_SPEAK_THRESHOLD = 0.3  # ë§í•˜ê¸° ê¸°ì¤€
        self.MAR_OPEN_THRESHOLD = 0.5   # ì… í¬ê²Œ ì—´ë¦¼
        
        # ì•ˆì •í™”ë¥¼ ìœ„í•œ ë²„í¼
        self.ear_buffer = deque(maxlen=5)
        self.mar_buffer = deque(maxlen=5)
    
    def calculate_ear(self, landmarks, eye_indices):
        """Eye Aspect Ratio ê³„ì‚°"""
        points = np.array([
            [landmarks[i].x, landmarks[i].y]
            for i in eye_indices
        ])
        
        # ìˆ˜ì§ ê±°ë¦¬
        A = np.linalg.norm(points[1] - points[5])
        B = np.linalg.norm(points[2] - points[4])
        
        # ìˆ˜í‰ ê±°ë¦¬
        C = np.linalg.norm(points[0] - points[3])
        
        ear = (A + B) / (2.0 * C)
        return ear
    
    def calculate_mar(self, landmarks, mouth_indices):
        """Mouth Aspect Ratio ê³„ì‚°"""
        points = np.array([
            [landmarks[i].x, landmarks[i].y]
            for i in mouth_indices
        ])
        
        # ìˆ˜ì§ ê±°ë¦¬
        A = np.linalg.norm(points[1] - points[7])
        B = np.linalg.norm(points[2] - points[6])
        C = np.linalg.norm(points[3] - points[5])
        
        # ìˆ˜í‰ ê±°ë¦¬
        D = np.linalg.norm(points[0] - points[4])
        
        mar = (A + B + C) / (3.0 * D)
        return mar
    
    def detect_ventilator(self, frame, face_bbox):
        """
        ì¸ê³µí˜¸í¡ê¸° ê²€ì¶œ
        - ì–¼êµ´ ì•„ë˜ ì˜ì—­ì—ì„œ ë§ˆìŠ¤í¬/íŠœë¸Œ ê²€ì¶œ
        - ìƒ‰ìƒ ê¸°ë°˜ + í˜•íƒœ ë¶„ì„
        """
        x1, y1, x2, y2 = face_bbox
        h, w = frame.shape[:2]
        
        # ì–¼êµ´ ì•„ë˜ ì˜ì—­ í¬ë¡­
        mouth_region_y1 = int(y1 + (y2 - y1) * 0.6)
        mouth_region_y2 = min(int(y2 + (y2 - y1) * 0.3), h)
        mouth_region = frame[mouth_region_y1:mouth_region_y2, x1:x2]
        
        if mouth_region.size == 0:
            return False, 0.0
        
        # HSV ë³€í™˜
        hsv = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2HSV)
        
        # í°ìƒ‰/ì²­ë¡ìƒ‰ ë§ˆìŠ¤í¬ ê²€ì¶œ (ì˜ë£Œìš© ë§ˆìŠ¤í¬ ìƒ‰ìƒ)
        lower_white = np.array([0, 0, 200])
        upper_white = np.array([180, 30, 255])
        mask_white = cv2.inRange(hsv, lower_white, upper_white)
        
        lower_blue = np.array([90, 50, 50])
        upper_blue = np.array([130, 255, 255])
        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)
        
        mask_combined = cv2.bitwise_or(mask_white, mask_blue)
        
        # ë§ˆìŠ¤í¬ ì˜ì—­ ë¹„ìœ¨
        mask_ratio = np.count_nonzero(mask_combined) / mask_combined.size
        
        has_ventilator = mask_ratio > 0.3  # 30% ì´ìƒì´ë©´ í˜¸í¡ê¸° ì°©ìš©
        
        return has_ventilator, mask_ratio
    
    def analyze_expression(self, landmarks):
        """
        ì–¼êµ´ í‘œì • ë¶„ì„ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        - ë” ì •í™•í•œ ë¶„ì„ì€ FER ëª¨ë¸ ì‚¬ìš©
        """
        # ëˆˆì¹ ë†’ì´ (ì°¡ê·¸ë¦¼ ê²€ì¶œ)
        left_eyebrow = landmarks[70].y
        right_eyebrow = landmarks[300].y
        eyebrow_avg = (left_eyebrow + right_eyebrow) / 2
        
        # ì…ê¼¬ë¦¬ (ì›ƒìŒ ê²€ì¶œ)
        left_mouth = landmarks[61].y
        right_mouth = landmarks[291].y
        mouth_corners_avg = (left_mouth + right_mouth) / 2
        
        # ê°„ë‹¨í•œ ê·œì¹™
        if eyebrow_avg < 0.35:  # ëˆˆì¹ì´ ì˜¬ë¼ê°
            return "surprised"
        elif mouth_corners_avg > 0.6:  # ì…ê¼¬ë¦¬ê°€ ë‚´ë ¤ê°
            return "sad"
        else:
            return "neutral"
    
    def analyze_face(self, frame, person_bbox):
        """
        ì–¼êµ´ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            frame: ì „ì²´ í”„ë ˆì„
            person_bbox: ì‚¬ëŒ BBox (x1, y1, x2, y2)
        
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        x1, y1, x2, y2 = map(int, person_bbox)
        
        # ì‚¬ëŒ ì˜ì—­ í¬ë¡­
        person_crop = frame[y1:y2, x1:x2]
        
        if person_crop.size == 0:
            return None
        
        # RGB ë³€í™˜ (MediaPipe ìš”êµ¬ì‚¬í•­)
        rgb_crop = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)
        
        # MediaPipe ì²˜ë¦¬
        results = self.face_mesh.process(rgb_crop)
        
        if not results.multi_face_landmarks:
            return None
        
        # ì²« ë²ˆì§¸ ì–¼êµ´ ë¶„ì„
        face_landmarks = results.multi_face_landmarks[0]
        
        # EAR ê³„ì‚° (ëˆˆ ìƒíƒœ)
        left_ear = self.calculate_ear(
            face_landmarks.landmark,
            self.LEFT_EYE
        )
        right_ear = self.calculate_ear(
            face_landmarks.landmark,
            self.RIGHT_EYE
        )
        avg_ear = (left_ear + right_ear) / 2
        self.ear_buffer.append(avg_ear)
        ear_smoothed = np.mean(self.ear_buffer)
        
        # MAR ê³„ì‚° (ì… ìƒíƒœ)
        mar = self.calculate_mar(
            face_landmarks.landmark,
            self.MOUTH_OUTER
        )
        self.mar_buffer.append(mar)
        mar_smoothed = np.mean(self.mar_buffer)
        
        # ëˆˆ ìƒíƒœ íŒë‹¨
        eyes_open = ear_smoothed > self.EAR_THRESHOLD
        
        # ì… ìƒíƒœ íŒë‹¨
        if mar_smoothed > self.MAR_OPEN_THRESHOLD:
            mouth_state = "wide_open"
        elif mar_smoothed > self.MAR_SPEAK_THRESHOLD:
            mouth_state = "speaking"
        else:
            mouth_state = "closed"
        
        # í‘œì • ë¶„ì„
        expression = self.analyze_expression(face_landmarks.landmark)
        
        # í˜¸í¡ê¸° ê²€ì¶œ
        has_ventilator, ventilator_conf = self.detect_ventilator(
            person_crop,
            (0, 0, x2-x1, y2-y1)  # í¬ë¡­ ë‚´ ìƒëŒ€ ì¢Œí‘œ
        )
        
        return {
            'face_detected': True,
            'eyes_open': eyes_open,
            'ear': ear_smoothed,
            'mouth_state': mouth_state,
            'mar': mar_smoothed,
            'expression': expression,
            'has_ventilator': has_ventilator,
            'ventilator_confidence': ventilator_conf,
            'landmarks': face_landmarks
        }
```

---

## ğŸ”— ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©

### 3ï¸âƒ£ **RealtimeDetector ìˆ˜ì •**

```python
# realtime_detector.py ìˆ˜ì •

from face_analyzer import FaceAnalyzer

class RealtimeDetector:
    def __init__(self, config, roi_regions):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # ì–¼êµ´ ë¶„ì„ê¸° ì¶”ê°€
        self.face_analyzer = FaceAnalyzer()
        self.enable_face_analysis = config.get('enable_face_analysis', False)
    
    def process_frame(self):
        """í”„ë ˆì„ ì²˜ë¦¬ (ì–¼êµ´ ë¶„ì„ ì¶”ê°€)"""
        ret, frame = self.cap.read()
        if not ret:
            return None
        
        current_time = time.time()
        detections = []
        face_analysis_results = {}
        
        # YOLO ì¶”ë¡  (ì„¤ì •ëœ ê°„ê²©ë§ˆë‹¤)
        if current_time - self.last_detection_time >= self.detection_interval:
            
            # YOLOë¡œ ì‚¬ëŒ ê²€ì¶œ
            results = self.model(frame, verbose=False)
            
            for result in results:
                boxes = result.boxes
                for box in boxes:
                    cls = int(box.cls[0])
                    conf = float(box.conf[0])
                    
                    if cls == self.person_class_id and conf >= self.confidence_threshold:
                        bbox = box.xyxy[0].cpu().numpy()
                        
                        detections.append({
                            'bbox': bbox,
                            'confidence': conf
                        })
                        
                        # ì–¼êµ´ ë¶„ì„ (ì˜µì…˜)
                        if self.enable_face_analysis:
                            face_result = self.face_analyzer.analyze_face(
                                frame, bbox
                            )
                            
                            if face_result:
                                face_analysis_results[tuple(bbox)] = face_result
            
            # ROI ì²´í¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            for roi in self.roi_regions:
                roi_id = roi['id']
                person_in_roi = False
                
                for detection in detections:
                    if self.is_person_in_polygon_roi(detection['bbox'], roi):
                        person_in_roi = True
                        break
                
                self.update_roi_state(roi_id, person_in_roi)
            
            self.last_detections = detections
            self.last_face_results = face_analysis_results
            self.last_detection_time = current_time
        
        # í”„ë ˆì„ì— ì‹œê°í™”
        annotated_frame = self.draw_detections_with_faces(
            frame,
            self.last_detections,
            self.last_face_results
        )
        
        return annotated_frame
    
    def draw_detections_with_faces(self, frame, detections, face_results):
        """BBox + ì–¼êµ´ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
        frame_copy = frame.copy()
        
        for detection in detections:
            bbox = detection['bbox']
            x1, y1, x2, y2 = map(int, bbox)
            
            # ì‚¬ëŒ BBox ê·¸ë¦¬ê¸°
            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), (255, 0, 0), 2)
            
            # ì–¼êµ´ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            face_result = face_results.get(tuple(bbox))
            
            if face_result and face_result['face_detected']:
                # í…ìŠ¤íŠ¸ ì¤€ë¹„
                info_lines = [
                    f"Eyes: {'Open' if face_result['eyes_open'] else 'Closed'}",
                    f"Mouth: {face_result['mouth_state']}",
                    f"Expr: {face_result['expression']}",
                ]
                
                if face_result['has_ventilator']:
                    info_lines.append(f"Ventilator: Yes ({face_result['ventilator_confidence']:.2f})")
                
                # í…ìŠ¤íŠ¸ ë°°ê²½
                text_y = y1 - 10
                for i, line in enumerate(info_lines):
                    text_y_pos = text_y - (len(info_lines) - i) * 20
                    cv2.putText(
                        frame_copy, line,
                        (x1, text_y_pos),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5, (0, 255, 0), 2
                    )
        
        return frame_copy
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” ì „ëµ

### **Jetson Orin ìµœì í™”**

#### 1ï¸âƒ£ **GPU ë©”ëª¨ë¦¬ ê´€ë¦¬**
```python
# TensorFlow Lite GPU Delegate ì‚¬ìš©
import tensorflow as tf

# GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    tf.config.experimental.set_memory_growth(gpus[0], True)
```

#### 2ï¸âƒ£ **ë³‘ë ¬ ì²˜ë¦¬**
```python
# YOLOì™€ Face Analysisë¥¼ ë²ˆê°ˆì•„ ì‹¤í–‰
frame_count = 0

if frame_count % 2 == 0:
    # YOLO ì¶”ë¡ 
    yolo_results = model(frame)
else:
    # ì–¼êµ´ ë¶„ì„ (ì´ì „ YOLO ê²°ê³¼ ì‚¬ìš©)
    if previous_detections:
        for bbox in previous_detections:
            face_result = face_analyzer.analyze_face(frame, bbox)

frame_count += 1
```

#### 3ï¸âƒ£ **í•´ìƒë„ ì¡°ì •**
```python
# ì–¼êµ´ ë¶„ì„ì€ ë‚®ì€ í•´ìƒë„ë¡œ
person_crop_resized = cv2.resize(person_crop, (320, 320))
face_result = face_analyzer.analyze_face(person_crop_resized, ...)
```

#### 4ï¸âƒ£ **ì„ íƒì  ë¶„ì„**
```python
# ROI ë‚´ë¶€ ì‚¬ëŒë§Œ ì–¼êµ´ ë¶„ì„
if person_in_roi:
    face_result = face_analyzer.analyze_face(frame, bbox)
```

---

## âš™ï¸ ì„¤ì • íŒŒì¼ (config.json)

```json
{
  "yolo_model": "yolov8n.pt",
  "camera_source": 0,
  "frame_width": 1280,
  "frame_height": 720,
  "confidence_threshold": 0.5,
  "detection_interval_seconds": 1.0,
  
  "enable_face_analysis": true,
  "face_analysis_interval": 2.0,
  "face_analysis_roi_only": true,
  
  "ear_threshold": 0.21,
  "mar_speak_threshold": 0.3,
  "mar_open_threshold": 0.5,
  "ventilator_detection_threshold": 0.3
}
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ (Jetson Orin Nano)

| ëª¨ë“œ | YOLO FPS | Face Analysis FPS | ì´ FPS |
|------|----------|-------------------|--------|
| YOLOë§Œ | 30-40 | - | 30-40 |
| ëª¨ë‘ í™œì„±í™” (ë²ˆê°ˆì•„) | 30 | 15 | 25-30 |
| ROIë§Œ ë¶„ì„ | 35 | 10-15 | 30-35 |

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜
```bash
cd /home/user/yolo_roi_detector

# MediaPipe ì„¤ì¹˜
pip install mediapipe-gpu

# ìƒˆ íŒŒì¼ ìƒì„±
# - face_analyzer.py (ìœ„ FaceAnalyzer í´ë˜ìŠ¤)
# - realtime_detector.py ìˆ˜ì •
```

### ì‹¤í–‰
```bash
# config.json ìˆ˜ì •
# "enable_face_analysis": true ì„¤ì •

# Streamlit ì‹¤í–‰
streamlit run streamlit_app.py
```

---

## ğŸ¯ ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´

### 1ï¸âƒ£ **í‘œì • ë¶„ì„ ì •í™•ë„ í–¥ìƒ**
```bash
# FER (Facial Expression Recognition) ì‚¬ìš©
pip install fer

from fer import FER
detector = FER(mtcnn=False)  # MediaPipe ëœë“œë§ˆí¬ ì‚¬ìš©
emotions = detector.detect_emotions(face_crop)
```

### 2ï¸âƒ£ **Gaze Tracking (ì‹œì„  ì¶”ì )**
```python
# MediaPipe ì•„ì´ë¦¬ìŠ¤ ëœë“œë§ˆí¬ í™œìš©
# ì¢Œìš° ì‹œì„  ë°©í–¥ ê²€ì¶œ
```

### 3ï¸âƒ£ **Head Pose Estimation (ë¨¸ë¦¬ ë°©í–¥)**
```python
# 3D ëœë“œë§ˆí¬ë¡œ ì–¼êµ´ íšŒì „ ê°ë„ ê³„ì‚°
# Pitch, Yaw, Roll
```

### 4ï¸âƒ£ **Drowsiness Detection (ì¡¸ìŒ ê°ì§€)**
```python
# EARì´ ë‚®ì€ ìƒíƒœê°€ 3ì´ˆ ì´ìƒ ì§€ì†
# + í•˜í’ˆ ê²€ì¶œ (MAR ë†’ìŒ)
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **MediaPipe**: https://google.github.io/mediapipe/
- **Face Mesh Guide**: https://google.github.io/mediapipe/solutions/face_mesh
- **EAR Paper**: Real-Time Eye Blink Detection using Facial Landmarks
- **MAR Paper**: Driver Yawning Detection Based on Mouth Aspect Ratio

---

## âœ… ìš”ì•½

**ìµœì  êµ¬ì„± (Jetson Orin)**:
1. âœ… YOLO (ì‚¬ëŒ ê²€ì¶œ) - ê¸°ì¡´ ìœ ì§€
2. âœ… MediaPipe Face Mesh (ì–¼êµ´ ë¶„ì„) - ì¶”ê°€
3. âœ… EAR/MAR ì•Œê³ ë¦¬ì¦˜ (ëˆˆ/ì… ìƒíƒœ)
4. âœ… ìƒ‰ìƒ ê¸°ë°˜ í˜¸í¡ê¸° ê²€ì¶œ
5. âœ… ì„ íƒì  ì²˜ë¦¬ (ROI ë‚´ë¶€ë§Œ)

**ì˜ˆìƒ ì„±ëŠ¥**: 25-35 FPS (ì „ì²´ íŒŒì´í”„ë¼ì¸)

**ê°œë°œ ì‹œê°„**: 2-3ì¼ (í†µí•© ë° í…ŒìŠ¤íŠ¸ í¬í•¨)
